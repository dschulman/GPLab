\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\newcommand{\dif}{\mathop{}\!{d}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\real}{\mathbb{R}}
\DeclareMathOperator{\diag}{diag}
\DeclarePairedDelimiter{\Det}{\lvert}{\rvert}

\title{Notes on Exact GP Regression with Replication}
\author{Dan Schulman}

\begin{document}
\maketitle

Let $y \in \real^{\infty \times n}$ be a vector of observations, where each observation $y_i \in \real^{m_i}$ is a vector of $m_i$ replicates.
Let $x \in \mathcal{X}^n$ be a vector of input variables, where $\mathcal{X}$ is an arbitrary set.

Let $f \in \real^n$ be latent means.
Let $\sigma^2 \in \real_{>0}$ be the observation variance.
Assume $y_{ij}$ is drawn from a normal distribution:

\begin{equation}
    p(y_{ij} \mid f_i, \sigma^2) = N(y_{ij} \mid f_i, \sigma^2)
\end{equation}

Let $\mu \in \real^n$ be a prior mean, which may be constant, or $\mu_i$ may be a function of $x_i$.
Let $k(x_i, x_j; \theta)$ be a positive definite covariance function on $\mathcal{X}$ with parameters $\theta$.
Let $K$ be a $n \times n$ covariance matrix such that $K_{ij} = k(x_i, x_j; \theta)$.
Assume $f$ has a gaussian process prior, so that when instatiated at $x$:

\begin{equation}
    p(f \mid \mu, \theta) = N(f \mid \mu, K)
\end{equation}

First, consider $p(y \mid f, \sigma^2)$.
Let $u \in \real^n$ and $v \in \real^n$ be the sample means and (uncorrected) sample variances of each $y_i$.

\begin{equation}
    u_i = \frac{1}{m_i} \sum_{j=1}^{m_i} y_{ij}
\end{equation}

\begin{equation}
    v_i = \frac{1}{m_i} \sum_{j=1}^{m_i} (y_{ij} - u_i)^2
\end{equation}

Since $u_i$ and $v_i$ are sufficient statistics, we can write $p(y_i \mid f_i, \sigma^2)$ using only these, regardless of the number of replicates.
In the last step, note that $\sum_{j=1}^{m_i} (y_{ij} - u_i) = 0$.

\begin{equation}
    p(y_i \mid f_i, \sigma^2)
    =
    (2 \pi \sigma^2)^{-\frac{1}{2} m_i} 
    \exp\left(
        -\frac{m_i}{2\sigma^2} \left(
            (u_i - f_i)^2 + v_i
        \right)
    \right)
\end{equation}

Note that this contains a normal distribution on $u_i$.
Using this, we can write $p(y \mid f, \sigma^2)$ as a product of a normal distribution and a term to account for added variance.

\begin{gather}
    p(y \mid f, \sigma^2) = V \cdot p(u \mid f, \sigma^2)
    \\
    p(u \mid f, \sigma^2) = N(u \mid f, \sigma^2 M^{-1})
    \\
    V = (2 \pi \sigma^2)^{-\frac{1}{2}(\one^T m - n)} \Det{M}^{-\frac{1}{2}} \exp\left(-\frac{1}{2\sigma^2} m^T v\right)
    \\
    M = \diag(m)
\end{gather}

We are essentially done!
This is almost equivalent to a standard GP on $u$ rather than $y$, and standard formulas will all apply with slight modifications.
The correction term $V$ appears in the marginal likelihood:

\begin{align}
    p(y \mid \mu, \sigma^2, \theta)
    &=
    \int_{\real^n} p(y \mid f, \sigma^2) \; p(f \mid \mu, \theta) \; \dif f
    \\ &=
    V \int_{\real^n} p(u \mid f, \sigma^2) \; p(f \mid \mu, \theta) \; \dif f
    \\ &=
    V \cdot p(u \mid \mu, \sigma^2, \theta)
    \\
    p(u \mid \mu, \sigma^2, \theta)
    &=
    N(u \mid \mu, K + \sigma^2 M^{-1})
\end{align}

However, it vanishes in the posterior of $f$:

\begin{align}
    p(f \mid y, \mu, \sigma^2, \theta)
    &=
    \frac{p(y \mid f, \sigma^2) \; p(f \mid \mu, \theta)}{p(y \mid \mu, \sigma^2, \theta)}
    \\ &=
    \frac{p(u \mid f, \sigma^2) \; p(f \mid \mu, \theta)}{p(u \mid \mu, \sigma^2, \theta)}
    \\ &=
    p(f \mid u, \mu, \sigma^2, \theta)
\end{align}

By standard formula of the posterior of a multivariate normal:

\begin{gather}
    \mathbb{E}[f \mid y, \mu, \sigma^2, \theta) =
    (K^{-1} + \sigma^{-2} M)^{-1} (K^{-1} \mu + \sigma^{-2} M u)
    \\
    \mathbb{V}[f \mid y, \mu, \sigma^2, \theta] =
    (K^{-1} + \sigma^{-2} M)^{-1}
\end{gather}

Since only the posterior is needed for the posterior predictive distribution, $V$ does not appear there either.
Let $x_*$ be a vector of test input points.
Let $\mu_*$ be the values of the mean function at $x_*$.
Let $K_{*(ij)} = k(x_i, x_{*(j)}; \theta)$.
Let $K_{**(ij)} = k(x_{*(i)}, x_{*(j)}; \theta)$.

\begin{gather}
    \mathbb{E}[f_* \mid y, \mu, \sigma^2, \theta] =
    \mu_* + K_*^T (K + \sigma^2 M^{-1})^{-1} (u - \mu)
    \\
    \mathbb{V}[f_* \mid y, \mu, \sigma^2, \theta] =
    K_{**} - K_*^T (K + \sigma^2 M^{-1})^{-1} K_*
\end{gather}

\end{document}